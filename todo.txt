REFACTORING:
> add train report
- plot critic error
- plot actor error
- use events in mind and algorithms
- implement new algorithm for test

- save training state every 10% to be able restore it if need -- use 009/bak/0010000 folder
- use 009 config and try to fit the best metaparameters
- train with .2 noise rate
- train with actor LR/=2
- manage LR -- use lr_method fo actor and critic to establish learning process

THEN:
- use relative ball coords and speed in scorpion.inputs (rel to scorpion basis)
- init ball in random cube above the agent
- train to move target to ball (dist ball-target)
- train to move target to fixed ball if failed to mobile one
- report eval actions to be shure it works (with corresponded actuators names)
- fix demo and eval while training scorpion (see world.run_episode and agent.predict_actions)
- demo scorpion
- simulate MVP model (arm and spoon)4

LATER:
- add BN to improve stability and accelerate learning
- use task to encapsulate trainig goals (episode_jpos_method, reward_method, done, etc)
- train: freeze or remove the ball and the turret
- save params to out_path/params.txt

DONE:
+ use new reports
+ show reports in browser
+ fix warnings in termninal
+ move to python 3.5
+ install http server
x train to catch the ball (add ball.velocity to inputs) reward := z^2 - |x| - |y| if z>2, done := z<1
+ train scorpion with pretrained tentacle
+ demo pretrained tentacle
+ format alg_obs
+ fix eval (maybe use step_agent instead step)
+ train: save - restore weights
+ use state_dim instead of obs_dim -- they are different in agent, as to state is flatten but obs are not
+ refactore -- clear code, fix up dependences
+ refactore training -- use callbacks from alg
+ train: add reward method
+ refactore (see todos in code)
+ train: controll tagret coords
+ train tentacle
+ correctly calculate agent.state_dim
+ simplify provide_inputs method
+ run the scorpion
+ use mouse to control target # mx, my = glfw.get_cursor_pos(window)
+ draw diagramm for Noise, Qmax and Rewards (w/o noise) depended on Episodes and Time
+ reporter
+ print time estimation
+ print experiment summary every 100 episodes
+ run demo experiments
+ use pickle to save/restore algorithm state: episodes, replay buffer
    http://stackoverflow.com/questions/6568007/how-do-i-save-and-restore-multiple-variables-in-python
    http://python.about.com/od/pythonstandardlibrary/a/pickle_intro.htm
    https://www.safaribooksonline.com/library/view/head-first-python/9781449397524/ch04.html
+ add experiment name to the mujoco window title
+ run experiment with training the tentacle
