TODO:
- train to move target to ball (dist ball-target)
- fix demo and eval while training scorpion (see world.run_episode and agent.predict_actions)
- demo scorpion
- simulate MVP model (arm and spoon)

LATER:
- train: freeze or remove the ball and the turret
- save params to out_path/params.txt
- move to python 3.4

DONE:
x train to catch the ball (add ball.velocity to inputs) reward := z^2 - |x| - |y| if z>2, done := z<1
+ train scorpion with pretrained tentacle
+ demo pretrained tentacle
+ format alg_obs
+ fix eval (maybe use step_agent instead step)
+ train: save - restore weights
+ use state_dim instead of obs_dim -- they are different in agent, as to state is flatten but obs are not
+ refactore -- clear code, fix up dependences
+ refactore training -- use callbacks from alg
+ train: add reward method
+ refactore (see todos in code)
+ train: controll tagret coords
+ train tentacle
+ correctly calculate agent.state_dim
+ simplify provide_inputs method
+ run the scorpion
+ use mouse to control target # mx, my = glfw.get_cursor_pos(window)
+ draw diagramm for Noise, Qmax and Rewards (w/o noise) depended on Episodes and Time
+ reporter
+ print time estimation
+ print experiment summary every 100 episodes
+ run demo experiments
+ use pickle to save/restore algorithm state: episodes, replay buffer
    http://stackoverflow.com/questions/6568007/how-do-i-save-and-restore-multiple-variables-in-python
    http://python.about.com/od/pythonstandardlibrary/a/pickle_intro.htm
    https://www.safaribooksonline.com/library/view/head-first-python/9781449397524/ch04.html
+ add experiment name to the mujoco window title
+ run experiment with training the tentacle
